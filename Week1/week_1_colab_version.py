# -*- coding: utf-8 -*-
"""WEEK 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/186LS97oRR2aw3RrIuBBo0evfOHRDSntf

# **SETUP**
"""

import xml.etree.ElementTree as ET
import json
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from matplotlib import rc as rc
import scipy as sp
from scipy.stats import chi2, chi
import IPython
from matplotlib.animation import FFMpegWriter
rc('animation', html='jshtml')

from google.colab import drive
drive.mount('/content/drive')

"""# **VIDEO PLAYER**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import IPython.display

class VideoPlayer:
    def __init__(self, video_path):
        self.video_path = video_path
        self.cap = cv2.VideoCapture(video_path)
        if not self.cap.isOpened():
            raise ValueError(f"Could not open video file: {video_path}")

        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)

        # Close immediately if you only need the metadata
        self.cap.release()

    def plot(
        self,
        start_frame=0,
        end_frame=100,
        masks=None,
        overlay_color=(255, 0, 0),
        alpha=0.3,
        # --- NEW ARGS ---
        bounding_boxes_gt=None,      # list of lists of dicts (GT)
        bounding_boxes_pred=None,    # list of lists of dicts (Pred)
        gt_color=(0, 255, 0),        # BGR = green
        pred_color=(255, 0, 0),      # BGR = red
        bbox_thickness=5,
        save_file=None,
    ):
        """
        Displays an animation of the video frames within [start_frame, end_frame).
        Optionally applies:
        - A mask overlay (masks[i] over frames[i]) in overlay_color, alpha-blended.
        - Ground-truth bounding boxes in gt_color.
        - Predicted bounding boxes in pred_color.
        - Saves the animation to an MP4 or GIF file if save_file is not None.

        :param start_frame:   First frame index (inclusive).
        :param end_frame:     Last frame index (exclusive).
        :param masks:         Optional np.ndarray of shape (N, H, W), binary (0 or 1).
        :param overlay_color: (R, G, B) overlay color for masks (if frames are in RGB).
        :param alpha:         Blending factor in [0,1] for mask overlay.
        :param bounding_boxes_gt:   Optional list of lists of dicts for GT,
                                    e.g. bounding_boxes_gt[i] = [{"bbox": [x, y, w, h]}, ...].
        :param bounding_boxes_pred: Optional list of lists of dicts for Predictions,
                                    e.g. bounding_boxes_pred[i] = [{"bbox": [x, y, w, h]}, ...].
        :param gt_color:      (B, G, R) color for GT bounding boxes.
        :param pred_color:    (B, G, R) color for predicted bounding boxes.
        :param bbox_thickness: thickness of the bounding box lines.
        :param save_file:     If not None, path to save the animation (e.g. 'output.mp4' or 'output.gif').
        """
        frames = self.get_frames(start_frame, end_frame)
        if len(frames) == 0:
            print("No frames to display for the specified frame range.")
            return

        # 1) Validate masks if provided
        if masks is not None and len(masks) != len(frames):
            raise ValueError(
                f"Number of mask frames ({len(masks)}) must match "
                f"the number of video frames ({len(frames)})."
            )

        # 2) Validate bounding box lists if provided
        if bounding_boxes_gt is not None and len(bounding_boxes_gt) != len(frames):
            raise ValueError(
                f"Number of GT box lists ({len(bounding_boxes_gt)}) "
                f"must match number of video frames ({len(frames)})."
            )

        if bounding_boxes_pred is not None and len(bounding_boxes_pred) != len(frames):
            raise ValueError(
                f"Number of Pred box lists ({len(bounding_boxes_pred)}) "
                f"must match number of video frames ({len(frames)})."
            )

        overlay_c = np.array(overlay_color, dtype=np.float32)

        processed_frames = []
        for i, frame in enumerate(frames):
            # Convert to float32 for blending
            out_frame = frame.astype(np.float32)

            # 1) If masks are provided, blend them
            if masks is not None:
                mask = masks[i].astype(bool)
                out_frame[mask] = (1 - alpha) * out_frame[mask] + alpha * overlay_c

            # Convert to uint8 for drawing bounding boxes in OpenCV
            out_frame_uint8 = out_frame.astype(np.uint8)

            # 2) Draw ground-truth bounding boxes in green, if provided
            if bounding_boxes_gt is not None:
                for gt_box in bounding_boxes_gt[i]:
                    x, y, w, h = gt_box["bbox"]
                    cv2.rectangle(
                        out_frame_uint8,
                        (int(x), int(y)), (int(x + w), int(y + h)),
                        color=gt_color,
                        thickness=bbox_thickness
                    )

            # 3) Draw predicted bounding boxes in red, if provided
            if bounding_boxes_pred is not None:
                for pred_box in bounding_boxes_pred[i]:
                    x, y, w, h = pred_box["bbox"]
                    cv2.rectangle(
                        out_frame_uint8,
                        (int(x), int(y)), (int(x + w), int(y + h)),
                        color=pred_color,
                        thickness=bbox_thickness
                    )

            processed_frames.append(out_frame_uint8)

        processed_frames = np.array(processed_frames)

        # Now create the Matplotlib animation
        fig, ax = plt.subplots(figsize=(5, 5))
        img_display = ax.imshow(processed_frames[0])
        ax.set_title(f"Frame: {start_frame + 1}/{self.frame_count}")
        ax.axis("off")
        plt.tight_layout()

        def update(frame_idx):
            img_display.set_data(processed_frames[frame_idx])
            ax.set_title(f"Frame: {start_frame + frame_idx + 1}/{self.frame_count}")
            return (img_display,)

        anim = FuncAnimation(
            fig,
            update,
            frames=len(processed_frames),
            interval=1000 / self.fps if self.fps else 40,
            blit=True
        )

        # Save if needed
        if save_file is not None:
            from matplotlib.animation import FFMpegWriter, PillowWriter
            ext = save_file.lower().rsplit('.', 1)[-1]
            fps_for_save = int(self.fps) if self.fps else 30

            if ext == 'gif':
                writer = PillowWriter(fps=fps_for_save)
            else:
                writer = FFMpegWriter(fps=fps_for_save, metadata={'title': 'Video Output'})

            anim.save(save_file, writer=writer)
            print(f"Animation saved to {save_file}")

        IPython.display.display(anim)
        plt.close(fig)

    def animate_pixel_intensity(
        self,
        pixel_coord,
        start_frame=0,
        end_frame=100,
        color=False
    ):
        """
        Displays a real-time animation of both:
        - The video frames (with a box around the chosen pixel).
        - The pixel intensity evolution over time.

        :param pixel_coord: (x, y) pixel location in the frame.
        :param frame_start: First frame index (inclusive).
        :param frame_end:   Last frame index (exclusive).
        :param color:       If True, track (R, G, B) intensity. Otherwise, grayscale.
        """
        x, y = pixel_coord

        # Validate pixel coordinates
        if not (0 <= x < self.width and 0 <= y < self.height):
            raise ValueError(
                f"Pixel coordinates ({x}, {y}) are out of bounds for video size ({self.width}, {self.height})."
            )

        frames = []
        pixel_values = []

        cap = cv2.VideoCapture(self.video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        for frame_idx in range(start_frame, end_frame):
            if frame_idx >= self.frame_count:
                break

            ret, frame = cap.read()
            if not ret:
                break

            if color:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # pixel_values is Nx3
                pixel_values.append(frame_rgb[y, x])
                # Draw a box on a copy for display
                frame_disp = frame_rgb.copy()
                cv2.rectangle(
                    frame_disp, (x - 15, y - 15), (x + 15, y + 15),
                    color=(255, 0, 0), thickness=10
                )
            else:
                frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                # pixel_values is Nx1
                pixel_values.append([frame_gray[y, x]])
                # Draw a box on a color version for display
                frame_disp = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2RGB)
                cv2.rectangle(
                    frame_disp, (x - 15, y - 15), (x + 15, y + 15),
                    color=(255, 0, 0), thickness=10
                )

            frames.append(frame_disp)

        cap.release()

        if len(frames) == 0:
            print("No frames to display for the specified frame range.")
            return

        # Convert to numpy array for simpler indexing
        frames = np.array(frames)
        pixel_values = np.array(pixel_values)  # shape: (num_frames, 3) if color else (num_frames, 1)

        # Create the figure and axes
        fig, (ax_video, ax_plot) = plt.subplots(
            2, 1, figsize=(5, 5),
            gridspec_kw={'height_ratios': [3, 1]}
        )

        # Video axis
        img_display = ax_video.imshow(frames[0])
        ax_video.set_title(f"Frame: {start_frame}/{self.frame_count}")
        ax_video.axis('off')

        # Plot axis
        ax_plot.set_xlim(start_frame, start_frame + len(frames))
        if color:
            ax_plot.set_ylim(0, 255)
            ax_plot.set_ylabel("Pixel Intensity (R/G/B)")
            r_line, = ax_plot.plot([], [], lw=1, color='red', label='R')
            g_line, = ax_plot.plot([], [], lw=1, color='green', label='G')
            b_line, = ax_plot.plot([], [], lw=1, color='blue', label='B')
            lines = [r_line, g_line, b_line]
        else:
            ax_plot.set_ylim(0, 255)
            ax_plot.set_ylabel("Pixel Intensity (Gray)")
            gray_line, = ax_plot.plot([], [], lw=1, color='black', label='Gray')
            lines = [gray_line]

        ax_plot.set_xlabel("Frame")
        ax_plot.legend()
        plt.tight_layout()

        def init():
            for line in lines:
                line.set_data([], [])
            return (img_display, *lines)

        def update(frame_idx):
            img_display.set_data(frames[frame_idx])
            ax_video.set_title(f"Frame: {start_frame + frame_idx}/{self.frame_count}")

            # Update plot lines
            x_data = range(start_frame, start_frame + frame_idx + 1)
            for i, line in enumerate(lines):
                y_data = pixel_values[:frame_idx + 1, i]
                line.set_data(x_data, y_data)

            return (img_display, *lines)

        anim = FuncAnimation(
            fig,
            update,
            frames=len(frames),
            init_func=init,
            interval=1000 / self.fps if self.fps else 40,
            blit=True
        )

        IPython.display.display(anim)
        plt.close(fig)

    @staticmethod
    def plot_video(
        frames,
        masks=None,
        overlay_color=(255, 0, 0),
        alpha=0.3,
        bounding_boxes_per_frame=None,
        bbox_color=(0, 255, 0),
        bbox_thickness=2,
        fps=30,
        save_file=None
    ):
        """
        Displays an animation of the given frames (RGB or grayscale).
        Optionally applies:
        - A mask overlay (masks[i] over frames[i])
        - Bounding boxes from bounding_boxes_per_frame[i]
        - Saves the animation to MP4 or GIF if save_file is specified.
            (File extension determines the format.)

        :param frames: np.ndarray of shape (N, H, W, 3) [RGB]
                    or (N, H, W) [grayscale].
        :param masks:  np.ndarray of shape (N, H, W), binary (0 or 1). Default None.
        :param overlay_color: (R, G, B) color for overlay if frames are in RGB.
        :param alpha:  Blend factor in [0,1] for mask overlay.
        :param bounding_boxes_per_frame: list of lists of bounding boxes.
                Each element bounding_boxes_per_frame[i] should be a list of dicts,
                e.g. [{'bbox': (x, y, w, h)}, ...].
        :param bbox_color: (B, G, R) color for bounding boxes (OpenCV format).
        :param bbox_thickness: thickness of the bounding box lines.
        :param fps: frames per second for the animation.
        :param save_file: If not None, path to save (e.g., 'output.mp4' or 'output.gif').
        """
        if len(frames) == 0:
            print("No frames to display.")
            return

        # Check consistency if masks are provided
        if masks is not None and len(masks) != len(frames):
            raise ValueError(
                f"Number of mask frames ({len(masks)}) must match "
                f"the number of video frames ({len(frames)})."
            )

        # Check consistency if bounding boxes are provided
        if bounding_boxes_per_frame is not None and len(bounding_boxes_per_frame) != len(frames):
            raise ValueError(
                f"Number of bounding-box lists ({len(bounding_boxes_per_frame)}) "
                f"must match the number of video frames ({len(frames)})."
            )

        processed_frames = []
        overlay_c = np.array(overlay_color, dtype=np.float32)

        for i, frame in enumerate(frames):
            # If grayscale, convert to 3-channel
            if frame.ndim == 2:
                frame = np.stack([frame, frame, frame], axis=-1).astype(np.uint8)
            else:
                frame = frame.astype(np.uint8)

            out_frame = frame.astype(np.float32)

            # 1) If masks are provided, blend them
            if masks is not None:
                mask = masks[i].astype(bool)
                out_frame[mask] = (1 - alpha) * out_frame[mask] + alpha * overlay_c

            # 2) Draw bounding boxes if provided
            if bounding_boxes_per_frame is not None:
                out_frame_uint8 = out_frame.astype(np.uint8)
                for bbox_info in bounding_boxes_per_frame[i]:
                    x, y, w, h = bbox_info['bbox']
                    cv2.rectangle(
                        out_frame_uint8,
                        (int(x), int(y)), (int(x + w), int(y + h)),
                        color=bbox_color,
                        thickness=bbox_thickness
                    )
                out_frame = out_frame_uint8.astype(np.float32)

            processed_frames.append(out_frame.astype(np.uint8))

        processed_frames = np.array(processed_frames)

        # Create figure for animation
        fig, ax = plt.subplots(figsize=(5, 5))
        img_display = ax.imshow(processed_frames[0])
        ax.axis("off")
        plt.tight_layout()

        def update(frame_idx):
            img_display.set_data(processed_frames[frame_idx])
            ax.set_title(f"Frame: {frame_idx + 1}/{len(processed_frames)}")
            return (img_display,)

        anim = FuncAnimation(
            fig,
            update,
            frames=len(processed_frames),
            interval=1000 / fps if fps else 40,
            blit=True
        )

        # Optionally save to MP4 or GIF, based on file extension
        if save_file is not None:
            from matplotlib.animation import FFMpegWriter, PillowWriter
            ext = save_file.lower().rsplit('.', 1)[-1]
            if ext == 'gif':
                # Save as GIF using PillowWriter
                writer = PillowWriter(fps=fps)
            else:
                # Default to MP4 using FFMpegWriter
                writer = FFMpegWriter(fps=fps, metadata={'title': 'Video Output'})

            anim.save(save_file, writer=writer)
            print(f"Animation saved to {save_file}")

        IPython.display.display(anim)
        plt.close(fig)



    def get_frame(self, frame_idx):
        """
        Returns a single frame in RGB at the specified index.
        """
        cap = cv2.VideoCapture(self.video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise IndexError(f"Frame index {frame_idx} is out of range or not readable.")
        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    def get_frames(self, start_frame, end_frame):
        """
        Returns all frames in RGB from start_frame (inclusive) to end_frame (exclusive).
        """
        if start_frame >= self.frame_count:
            return np.array([])

        cap = cv2.VideoCapture(self.video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        frames = []
        for frame_idx in range(start_frame, end_frame):
            if frame_idx >= self.frame_count:
                break
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame_rgb)
        cap.release()
        return np.array(frames)

video_path = '/content/drive/MyDrive/## MCV/C6/DATA/week1/vdo.avi'
video_player = VideoPlayer(video_path)

# video_player.plot(start_frame=861, end_frame=921)

# video_player.animate_pixel_intensity((550, 420), start_frame=861, end_frame=921, color=True)

"""# **ANNOTATION**"""

class GTAnnotationHandler:
    def __init__(self, xml_path):
        """
        Initializes the handler by parsing the XML and building an internal
        representation of ground-truth bounding boxes.
        """
        self.xml_path = xml_path
        # Dictionary: frame -> list of annotations
        self.gt_by_frame = {}
        # Also keep track of distinct labels => category_ids
        # For simplicity, assume you only have 'car', 'bike' from the snippet
        self.label_to_id = {}
        self.label_to_id["car"] = 1
        self.label_to_id["bike"] = 1

        # Parse on init
        self._parse_xml()

    def _parse_xml(self):
        tree = ET.parse(self.xml_path)
        root = tree.getroot()

        # Typically <annotations> is the root, then <track> are child elements
        for track in root.findall("track"):
            label = track.attrib["label"]  # e.g. "car" or "bike"
            if label not in self.label_to_id:
                # If you have more labels, handle them here or assign new IDs
                self.label_to_id[label] = len(self.label_to_id) + 1

            track_id = track.attrib["id"]

            for box in track.findall("box"):
                frame_str = box.attrib["frame"]
                xtl = float(box.attrib["xtl"])
                ytl = float(box.attrib["ytl"])
                xbr = float(box.attrib["xbr"])
                ybr = float(box.attrib["ybr"])
                outside = box.attrib["outside"]  # might be "0" or "1"

                # By default, assume parked = False unless we find <attribute name="parked">true</attribute>
                parked = False
                for attr_node in box.findall("attribute"):
                    if attr_node.attrib.get("name") == "parked":
                        # If text is "true", mark parked = True
                        if attr_node.text.strip().lower() == "true":
                            parked = True

                # Convert to your desired [x, y, w, h]
                x = xtl
                y = ytl
                w = xbr - xtl
                h = ybr - ytl

                frame_idx = int(frame_str)
                cat_id = self.label_to_id[label]

                if parked or outside == "1":
                    # 'outside' means the object is not visible in this frame
                    # so you might choose to skip it
                    continue

                annotation = {
                    "bbox": [x, y, w, h],
                    "category_id": cat_id,
                    "track_id": track_id
                }

                if frame_idx not in self.gt_by_frame:
                    self.gt_by_frame[frame_idx] = []
                self.gt_by_frame[frame_idx].append(annotation)

    def get_gt_boxes_for_frame(self, start_frame, end_frame):
        """
        Return a list of bounding-box dicts for the given frame index.
        Each dict has {"bbox": [x, y, w, h], "category_id": ..., "track_id": ...}
        """
        result = []
        for i in range(start_frame, end_frame):
            result.append(self.gt_by_frame.get(i, []))
        return result

gt_handler = GTAnnotationHandler("/content/drive/MyDrive/## MCV/C6/DATA/week1/ai_challenge_s03_c010-full_annotation.xml")
gt_boxes = gt_handler.get_gt_boxes_for_frame(start_frame=861, end_frame=921)
# print(gt_boxes)
# video_player.plot(start_frame=0, end_frame=100, bounding_boxes_per_frame=gt_boxes)

"""# **EVALUATOR**"""

from xml.dom import minidom

def save_predictions(predictions, xml_path, width=1920, height=1080):
    """
    Saves a list-of-lists of bounding boxes into an XML file matching the format:

    <annotations>
      <meta width="..." height="..."/>
      <frame number="0">
        <box frame="0" xtl="..." ytl="..." xbr="..." ybr="..." outside="0" occluded="0" keyframe="0"/>
        ...
      </frame>
      ...
    </annotations>

    :param predictions: list of lists of bounding boxes.
                       Each outer list is one frame.
                       Each bounding box dict has {"bbox": [x, y, w, h], ...}.
    :param xml_path: path to the XML file to save.
    :param width: width of the frames (default 1920).
    :param height: height of the frames (default 1080).
    """
    root = ET.Element("annotations")

    # <meta width="..." height="..."/>
    meta_elem = ET.SubElement(root, "meta")
    meta_elem.set("width", str(width))
    meta_elem.set("height", str(height))

    # Create <frame> elements, each with <box> children
    for frame_idx, boxes_in_frame in enumerate(predictions):
        frame_elem = ET.SubElement(root, "frame")
        frame_elem.set("number", str(frame_idx))

        for box_dict in boxes_in_frame:
            x, y, w, h = box_dict["bbox"]
            xtl = x
            ytl = y
            xbr = x + w
            ybr = y + h

            box_elem = ET.SubElement(frame_elem, "box")
            box_elem.set("frame", str(frame_idx))
            box_elem.set("xtl", f"{xtl:.2f}")
            box_elem.set("ytl", f"{ytl:.2f}")
            box_elem.set("xbr", f"{xbr:.2f}")
            box_elem.set("ybr", f"{ybr:.2f}")
            box_elem.set("outside", "0")
            box_elem.set("occluded", "0")
            box_elem.set("keyframe", "0")

    # Now build a string and re-parse via minidom for pretty printing
    rough_string = ET.tostring(root, encoding="utf-8")
    reparsed = minidom.parseString(rough_string)
    pretty_xml_as_string = reparsed.toprettyxml(indent="  ")

    # Ensure the directory exists
    os.makedirs(os.path.dirname(xml_path), exist_ok=True)

    # Write out the pretty-printed XML
    with open(xml_path, "w", encoding="utf-8") as f:
        f.write(pretty_xml_as_string)

    print(f"Predictions saved to {xml_path}")

def read_predictions(xml_path):
    """
    Reads an XML file with the format:

    <annotations>
      <meta width="..." height="..."/>
      <frame number="0">
        <box frame="0" xtl="..." ytl="..." xbr="..." ybr="..." outside="0" occluded="0" keyframe="0"/>
        ...
      </frame>
      <frame number="1">
        <box .../>
        ...
      </frame>
      ...
    </annotations>

    And returns a list-of-lists of bounding boxes. The outer list is indexed
    by frame number. Each inner list has dictionaries with the form:

      {
        "bbox": [x, y, w, h],
        "outside": ...,
        "occluded": ...,
        "keyframe": ...
      }

    For frames not present in the file, the corresponding list will be empty.
    """
    tree = ET.parse(xml_path)
    root = tree.getroot()

    # We'll store boxes in a dictionary: frame_idx -> list of box dicts
    frames_dict = {}

    # 1) Loop over all <frame> elements
    for frame_elem in root.findall("frame"):

        frame_number_str = frame_elem.get("number", "0")
        frame_number = int(frame_number_str)

        # For each <box> in this frame
        for box_elem in frame_elem.findall("box"):
            # Parse the bounding box coordinates
            xtl = float(box_elem.get("xtl", 0.0))
            ytl = float(box_elem.get("ytl", 0.0))
            xbr = float(box_elem.get("xbr", 0.0))
            ybr = float(box_elem.get("ybr", 0.0))

            w = xbr - xtl
            h = ybr - ytl

            # Optionally parse other attributes if needed
            # outside = box_elem.get("outside", "0")
            # occluded = box_elem.get("occluded", "0")
            # keyframe = box_elem.get("keyframe", "0")

            box_dict = {
                "bbox": [xtl, ytl, w, h],
                "category_id": 1,
                "tracking_id": 0
            }

            if frame_number not in frames_dict:
                frames_dict[frame_number] = []
            frames_dict[frame_number].append(box_dict)

    # 2) Convert frames_dict to a list of lists, up to the max frame index
    if not frames_dict:
        return []  # no frames found
    print(frames_dict)

    max_frame = max(frames_dict.keys())
    predictions = []
    for i in range(max_frame + 1):
        # If the frame isn't in frames_dict, this yields an empty list
        r = frames_dict.get(i, [])
        print(r)
        predictions.append(r)

    return predictions

def to_coco_format(gt_boxes, pred_boxes):
    """
    Convert ground-truth and predicted boxes to COCO-format data structures.

    Returns:
        gt_coco (dict): COCO dataset-style dictionary for ground truth:
        {
            "images": [...],
            "annotations": [...],
            "categories": [...]
        }

        pred_coco (list): A list of prediction dicts in the format required by
                        COCO.loadRes():
        [
            {
            "image_id": i,
            "category_id": cat_id,
            "bbox": [x, y, w, h],
            "score": s
            },
            ...
        ]
    """
    # 1) Build images array
    num_frames = len(gt_boxes)
    images = []
    for i in range(num_frames):
        images.append({
            "id": i,
            "file_name": f"frame_{i}.jpg",
            # If you know a consistent width/height for all frames, you can store them here:
            # "height": 1080,
            # "width": 1920,
        })

    # 2) Collect all category_ids from GT to build categories
    #    Also build "annotations" for GT
    annotations = []
    cat_ids_found = set()
    ann_id_counter = 1

    for i, boxes_in_frame in enumerate(gt_boxes):
        for box in boxes_in_frame:
            x, y, w, h = box["bbox"]
            cat_id = box["category_id"]
            cat_ids_found.add(cat_id)

            # area is needed for COCO
            area = w * h

            annotations.append({
                "id": ann_id_counter,
                "image_id": i,
                "category_id": cat_id,
                "bbox": [x, y, w, h],
                "area": area,
                "iscrowd": 0,
                # "track_id": box["track_id"]  # not standard in COCO, but you could store it if needed
            })
            ann_id_counter += 1

    # 3) Build categories list
    #    We just produce "id" and "name" = str(category_id) for simplicity
    categories = []
    for cat_id in sorted(cat_ids_found):
        categories.append({
            "id": cat_id,
            "name": f"cat_{cat_id}"  # or a real label if you have one
        })

    # 4) Build ground-truth COCO dict
    gt_coco = {
        "images": images,
        "annotations": annotations,
        "categories": categories
    }

    # 5) Build predicted annotation list in the format COCO expects
    #    i.e. a list of dicts: {image_id, category_id, bbox, score}
    #    We'll flatten everything into one list (COCO detection format).
    pred_coco = []
    for i, boxes_in_frame in enumerate(pred_boxes):
        for box in boxes_in_frame:
            x, y, w, h = box["bbox"]
            cat_id = box["category_id"]
            # Use "score" if present, else default to 1.0
            s = box.get("score", 1.0)

            pred_coco.append({
                "image_id": i,
                "category_id": cat_id,
                "bbox": [x, y, w, h],
                "score": s
            })

    return gt_coco, pred_coco

def evaluate_map_50(gt_boxes, pred_boxes):
    """
    1) Convert GT & Predictions to COCO format.
    2) Use pycocotools to compute mAP at IoU=0.5.

    Returns: float representing AP@IoU=0.5
    """
    # Convert to COCO-like structures
    gt_coco_dict, pred_coco_list = to_coco_format(gt_boxes, pred_boxes)

    # Create a COCO object from ground-truth
    coco_gt = COCO()
    coco_gt.dataset = gt_coco_dict
    coco_gt.createIndex()

    # Load results (predictions) into COCO
    coco_dt = coco_gt.loadRes(pred_coco_list)

    # Evaluate
    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')

    # By default, COCO metrics average across IoU=0.5:0.95
    # We want to restrict it to only IoU=0.5
    coco_eval.params.iouThrs = [0.5]  # Single IoU threshold
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()

    # Typically, the "AP at IoU=0.5" is coco_eval.stats[0]
    # after we've forced iouThrs = [0.5].
    ap_50 = coco_eval.stats[0]
    ar_50 = coco_eval.stats[8]
    return ap_50, ar_50, 2*ap_50*ar_50/(ar_50+ap_50)

def evaluate_map_50_evolution(gt_boxes, pred_boxes, batch=50):
    """
    Computes the mAP@0.5 in batches of 'batch' frames throughout the video.

    :param gt_boxes: List of lists (one sub-list per frame), each sub-list containing
                     dicts with keys "bbox" = [x, y, w, h], "category_id", etc.
    :param pred_boxes: Same structure, but for predicted boxes (possibly with "score").
    :param batch: Size of each chunk of frames to compute mAP@0.5. Default is 50.
    :return: A list of AP@0.5 scores, one for each batch. Example: [0.65, 0.70, 0.62, ...]
    """
    total_frames = len(gt_boxes)
    ap_scores = []

    start_idx = 0
    while start_idx < total_frames:
        end_idx = min(start_idx + batch, total_frames)

        # Slice out the batch of frames
        gt_batch = gt_boxes[start_idx:end_idx]
        pred_batch = pred_boxes[start_idx:end_idx]

        # Evaluate mAP@0.5 on this slice
        ap_50 = evaluate_map_50(gt_batch, pred_batch)
        ap_scores.append(ap_50)

        start_idx = end_idx

    return ap_scores

print(evaluate_map_50(gt_boxes, gt_boxes))

"""# **TASK 1: Static Gaussian**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi
import os

class GaussianBackgroundClassifier:
    def __init__(self, video_path, method='mean'):
        """
        :param video_path: Path to the video file.
        :param method: 'mean' or 'median' (determines pixel-wise location/scale computation).
        """
        # Open video
        self.video_path = video_path
        self.cap = cv2.VideoCapture(video_path)
        if not self.cap.isOpened():
            raise ValueError(f"Could not open video file: {video_path}")

        self.method = method
        self.location = None
        self.scale = None
        self.global_cov = None
        self.global_cov_inv = None

        # Video metadata
        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)

        # We can close here if we only need metadata right now;
        # We'll reopen in fit/segment_foreground as needed.
        self.cap.release()

    def fit(self, num_init_frames=30, sample_grid=10):
        """
        Fit a Gaussian model to the first num_init_frames frames of the video.
        Pixel-wise location/scale are computed (mean or median), and
        a global 3x3 covariance is approximated by sampling on a grid.

        :param num_init_frames: Number of initial frames to use for fitting.
        :param sample_grid: Grid spacing used for sampling the normalized pixels
                            to compute the global covariance matrix.
        """
        # Sanity check
        if num_init_frames <= 0:
            raise ValueError("num_init_frames must be > 0.")

        # Reopen capture
        cap = cv2.VideoCapture(self.video_path)
        frames = []
        for i in range(num_init_frames):
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame_rgb)
        cap.release()

        if len(frames) == 0:
            raise ValueError("No frames were read for fitting.")

        frames_array = np.stack(frames, axis=0).astype(np.float32)  # (N, H, W, 3)
        num_frames, height, width, channels = frames_array.shape

        # 1) Pixel-wise location/scale
        if self.method == 'mean':
            self.location = np.mean(frames_array, axis=0)      # (H, W, 3)
            self.scale = np.var(frames_array, axis=0) + 1e-6   # (H, W, 3)
        elif self.method == 'median':
            # Do your 3x3 block approach
            section_height = height // 3
            section_width = width // 3
            self.location = np.zeros((height, width, channels), dtype=np.float32)
            self.scale = np.zeros((height, width, channels), dtype=np.float32)
            for i in range(3):
                for j in range(3):
                    h_start = i * section_height
                    h_end = (i + 1) * section_height if i < 2 else height
                    w_start = j * section_width
                    w_end = (j + 1) * section_width if j < 2 else width

                    section = frames_array[:, h_start:h_end, w_start:w_end, :]
                    median_loc = np.median(section, axis=0)
                    median_scale = np.median((section - median_loc) ** 2, axis=0) + 1e-6
                    self.location[h_start:h_end, w_start:w_end, :] = median_loc
                    self.scale[h_start:h_end, w_start:w_end, :] = median_scale

        # 2) Create normalized images => (pixel_val - location)/sqrt(scale)
        normalized_images = (frames_array - self.location) / (np.sqrt(self.scale) + 1e-8)  # shape (N,H,W,3)

        # 3) Approximate global 3x3 covariance using a grid sample
        self.global_cov = np.zeros((3, 3), dtype=np.float64)
        count = 0
        # sample_grid indicates how many points across width/height we skip
        x_coords = np.linspace(0, width - 1, sample_grid).astype(int)
        y_coords = np.linspace(0, height - 1, sample_grid).astype(int)
        for y in y_coords:
            for x in x_coords:
                # shape (N,3)
                pix_vals = normalized_images[:, y, x, :]
                # Cov of shape (3,3)
                c = np.cov(pix_vals, rowvar=False)
                self.global_cov += c
                count += 1
        self.global_cov /= count
        # Invert for Mahalanobis distance
        self.global_cov_inv = np.linalg.inv(self.global_cov)

    def segment_foreground(self, start_frame=0, end_frame=None, threshold=5, logits=False, plot=False):
        """
        Segment foreground from background for frames in [start_frame, end_frame).
        Returns a list of masks or a list of Z-score maps (if logits=True).

        :param start_frame: Index of first frame to process.
        :param end_frame: Index of last frame to process (exclusive).
                          If None, process until the end of the video.
        :param threshold: Z-score threshold for foreground vs. background.
        :param logits: If True, return Z-score map instead of mask.
        :param plot: If True, display a debug plot for the last processed frame.
        :return: List of masks (uint8) or Z-score arrays (float) for each frame.
        """
        if self.location is None or self.scale is None or self.global_cov_inv is None:
            raise ValueError("Model has not been fitted yet.")

        if end_frame is None:
            end_frame = self.frame_count
        if start_frame < 0 or end_frame <= start_frame:
            raise ValueError("Invalid frame range specified.")

        # Reopen the capture
        cap = cv2.VideoCapture(self.video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        results = []
        current_frame_idx = start_frame
        while current_frame_idx < end_frame:
            ret, frame = cap.read()
            if not ret:
                break

            frame_rgb = frame.astype(np.float32)
            # Convert to 'RGB' for consistency
            frame_rgb = cv2.cvtColor(frame_rgb, cv2.COLOR_BGR2RGB)

            # Normalize each pixel
            color_z_score = (frame_rgb - self.location) / (np.sqrt(self.scale) + 2.0)
            # Flatten
            color_z_score_flat = color_z_score.reshape(-1, 3)

            # Mahalanobis distance^2
            dist_sq = np.einsum('ij,jk,ik->i', color_z_score_flat, self.global_cov_inv, color_z_score_flat)
            z_score = np.sqrt(dist_sq).reshape(frame_rgb.shape[:2])  # (H, W)

            if logits:
                result_map = z_score.astype(np.float32)
            else:
                # threshold the z-score
                mask = (z_score >= threshold).astype(np.uint8)
                result_map = mask

            results.append(result_map)
            current_frame_idx += 1

        cap.release()

        # Optionally plot the last frame for debugging
        if plot and len(results) > 0:
            # We'll just plot the last processed frame
            final_result = results[-1]
            fig, axs = plt.subplots(1, 4, figsize=(16, 4))
            fig.suptitle(f"Frame {current_frame_idx-1}", y=0.8)

            # Original
            axs[0].imshow(frame_rgb.astype(np.uint8))
            axs[0].set_title('Frame')

            # Diff
            im_diff = axs[1].imshow(
                np.mean(np.abs((frame_rgb - self.location)), axis=-1),
                cmap='gray'
            )
            axs[1].set_title('Background Subtraction')
            fig.colorbar(im_diff, ax=axs[1], shrink=0.5)

            # Z-Score
            if logits:
                im_z = axs[2].imshow(final_result, vmin=0, vmax=10, cmap='gray')
            else:
                im_z = axs[2].imshow(z_score, vmin=0, vmax=10, cmap='gray')
            axs[2].set_title('Z-Score')
            fig.colorbar(im_z, ax=axs[2], shrink=0.5)

            # Mask
            if logits:
                axs[3].imshow(final_result > threshold, cmap='gray')
            else:
                axs[3].imshow(final_result, cmap='gray')
            axs[3].set_title('Mask')

            plt.tight_layout()
            plt.show()

        return np.array(results)

    def detect_objects(self, start_frame=0, end_frame=None, threshold=5):
        """
        Segments the entire video (or a given frame range),
        applies morphological filtering to reduce noise, and
        finds bounding boxes of connected components for each frame in batches.

        :param start_frame: Index of first frame to process (default 0).
        :param end_frame: Index of last frame to process (exclusive). If None, uses all frames.
        :param threshold: Z-score threshold for foreground vs. background.
        :param smoothing: Sigma used in the Gaussian filter for smoothing (default=3).
        :return: A list of lists of bounding boxes [x, y, w, h], one list per frame.
        """
        import scipy as sp
        import scipy.ndimage

        if end_frame is None:
            end_frame = self.frame_count

        max_batch_size = 60
        bounding_boxes_per_frame = []

        current_frame = start_frame
        foreground_mask = []
        while current_frame < end_frame:
            # Determine this batch's range
            batch_end = min(current_frame + max_batch_size, end_frame)

            # 1) Segment foreground in the current batch
            #    Request Z-score maps (logits=True) for morphological thresholding
            foreground_score_batch = self.segment_foreground(
                start_frame=current_frame,
                end_frame=batch_end,
                threshold=threshold,
                logits=True,   # We want the Z-score, not the binary mask
                plot=False
            )

            # 2) Morphological filtering & connected components for each frame in the batch
            for foreground_image in foreground_score_batch:
                # Apply Gaussian smoothing
                smoothed = sp.ndimage.gaussian_filter(foreground_image, sigma=12)
                smoothed = sp.ndimage.grey_dilation(smoothed, size=(5,5))

                # Threshold again to get a binary mask
                mask = (smoothed >= threshold)
                mask = sp.ndimage.binary_fill_holes(mask)
                foreground_mask.append(mask)
                # Find connected components
                num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask.astype(np.uint8), connectivity=8)

                frame_bboxes = []
                # stats: each row is [x, y, width, height, area]
                for label_idx in range(1, num_labels):  # label 0 is background
                    x, y, w, h, area = stats[label_idx]
                    # Potentially filter bounding boxes based on min area, etc.
                    if area < 400:
                        continue
                    # Store the dictionary in your chosen format
                    box_dict = {
                        "bbox": [x, y, w, h],
                        "category_id": 1,  # all assumed to be 'car'
                        "track_id": 0      # dummy track_id
                    }
                    frame_bboxes.append(box_dict)

                bounding_boxes_per_frame.append(frame_bboxes)

            # Advance to next batch
            current_frame = batch_end

        return bounding_boxes_per_frame, np.array(foreground_mask)


    def plot_fitted_background(self):
        """
        Optional visualization of the pixel-wise location & scale
        (only relevant for the method='mean' or 'median' approach).
        """
        if self.location is None or self.scale is None:
            raise ValueError("Model has not been fitted yet.")

        fig, axs = plt.subplots(1, 2, figsize=(10, 5))
        fig.suptitle(f"Gaussian Model [{self.method.capitalize()} Method]", y=0.8)
        axs[0].imshow(self.location.astype(int))
        axs[0].set_title('Location (RGB)')

        im_scale = axs[1].imshow(np.linalg.norm(self.scale, axis=-1), vmin=0, vmax=255, cmap='gray')
        axs[1].set_title('Scale (Norm)')
        fig.colorbar(im_scale, ax=axs[1], shrink=0.5)
        plt.tight_layout()
        plt.show()

gbc_mean = GaussianBackgroundClassifier(video_path, method='mean')
gbc_mean.fit(int(2141*0.25))
gbc_mean.plot_fitted_background()

gbc_median = GaussianBackgroundClassifier(video_path, method='median')
gbc_median.fit(int(2141*0.25))
gbc_median.plot_fitted_background()

foreground = gbc_median.segment_foreground(start_frame=2000, end_frame=2060, logits=False, plot=True, threshold=5.5)
# VideoPlayer.plot_video(np.clip(foreground, 0, 5)*255/5, save_file="/content/drive/MyDrive/## MCV/C6/DATA/week1/Results/static-gaussian-z-score.gif", fps=10)
VideoPlayer.plot_video(foreground*255, save_file="/content/drive/MyDrive/## MCV/C6/DATA/week1/Results/static-gaussian-comparison-masks.gif", fps=10)

gt = gt_handler.get_gt_boxes_for_frame(start_frame=861, end_frame=921)
objects, masks = gbc_median.detect_objects(start_frame=861, end_frame=921, threshold=3)
print(evaluate_map_50(gt, objects))
video_player.plot(start_frame=861, end_frame=921,  bounding_boxes_gt=gt, bounding_boxes_pred=objects, save_file="/content/drive/MyDrive/## MCV/C6/DATA/week1/Results/static-gaussian-result-1.gif")
# video_player.plot(start_frame=861, end_frame=921, bounding_boxes_per_frame=objects, bbox_thickness=5, save_file="/content/drive/MyDrive/## MCV/C6/DATA/week1/Results/static-gaussian-bboxes.gif")

gt = gt_handler.get_gt_boxes_for_frame(start_frame=2000, end_frame=2060)
objects, masks = gbc_median.detect_objects(start_frame=2000, end_frame=2060, threshold=3)
# print(evaluate_map_50(gt, objects))
VideoPlayer.plot_video(masks, save_file="/content/drive/MyDrive/## MCV/C6/DATA/week1/Results/static-gaussian-comparison-masks.gif")

thresholds = np.linspace(1, 6, 20)
maps = []
for t in thresholds:
    print(t)
    objects = gbc_mean.detect_objects(start_frame=861, end_frame=921, threshold=t)
    maps.append(evaluate_map_50(gt_boxes, objects))

plt.plot(thresholds, maps)
plt.xlabel("Sigma")
plt.ylabel("mAP")
plt.show()

# full_gt = gt_handler.get_gt_boxes_for_frame(start_frame=0, end_frame=2141)
# full_objects = gbc_mean.detect_objects(start_frame=0, end_frame=2141, threshold=3)
# print(evaluate_map_50(full_gt, full_objects))

full_objects = read_predictions("/content/drive/MyDrive/## MCV/C6/DATA/week1/static_gaussian_preds.xml")
full_gt = gt_handler.get_gt_boxes_for_frame(start_frame=0, end_frame=2141)
evaluate_map_50(full_gt, full_objects)

# full_objects = read_predictions("/content/drive/MyDrive/## MCV/C6/DATA/week1/adaptative_boxes_preds.xml")
# # print(full_objects)
# full_gt = gt_handler.get_gt_boxes_for_frame(start_frame=int(2141*0.25), end_frame=2141)
# evolution = evaluate_map_50_evolution(full_gt, full_objects, batch=50)
# evolution_smoothed = sp.ndimage.gaussian_filter(evolution, sigma=3)
# x = np.arange(0, 2141, 50)

# plt.figure(figsize=(6, 4))
# plt.plot(x, evolution, marker='o', label='raw')
# plt.plot(x, evolution_smoothed, marker='o', label='smoothed')
# plt.title('mAP@0.5 Evolution')
# plt.xlabel('Frame')
# plt.ylabel('mAP@0.5')
# plt.grid(True)
# plt.legend()
# plt.tight_layout()
# plt.show()

save_predictions(full_objects, "/content/drive/MyDrive/## MCV/C6/DATA/week1/static_gaussian_preds.xml")

"""# **TASK 2: Adaptative Gaussian**"""

class AdaGaussForegroundExtractor:
    def __init__(self, video_file, decay_factor=0.9):
        self.video_file = video_file
        self.location = None
        self.scale = None
        self.internal_index = 0
        self.decay_factor = decay_factor

    def fit(self):
        cap = cv2.VideoCapture(self.video_file)
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        ret, frame = cap.read()
        if not ret:
            raise IndexError(f"Frame index {0} is out of range or not readable.")
        self.location = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)
        self.scale = np.zeros_like(self.location)
        self.internal_index = 0
        cap.release()

    def predict_frame(self, frame, threshold=15, logits=False):
        if self.location is None or self.scale is None:
            raise ValueError("Model has not been fitted yet.")
        height, width, channels = frame.shape
        epsilon = 2
        color_z_score = (frame - self.location) / (np.sqrt(self.scale) + epsilon)
        z_score = np.linalg.norm(color_z_score, axis=-1)
        p_fg = chi.cdf(z_score, df=3)
        if logits:
            return z_score
        else:
            foreground_mask = (z_score >= threshold).astype(np.uint8)
            return foreground_mask

    def update(self, frame):
        if self.location is None or self.scale is None:
            raise ValueError("Model has not been fitted yet.")
        self.location = self.decay_factor * self.location + (1 - self.decay_factor) * frame
        self.scale = self.decay_factor * self.scale + (1 - self.decay_factor) * (frame - self.location) ** 2

    def predict(self, start_index, end_index, threshold=15, logits=False):
        self.fit()
        cap = cv2.VideoCapture(self.video_file)
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        for frame_idx in range(0, start_index):
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)
            self.update(frame_rgb)
        result = []
        for frame_idx in range(start_index, end_index):
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)
            result.append(self.predict_frame(frame_rgb, threshold, logits))
            self.update(frame_rgb)
        return result

"""# **RANDOM COMPUTATIONS**"""

import matplotlib.pyplot as plt

# Original thresholds (notice there are 11)
sigmas = [1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6]

# Original mAP values (10 values provided)
mAP_values = [0.05, 0.31, 0.7, 0.75, 0.81, 0.58, 0.35, 0.25, 0.2, 0.1]

# For plotting, we'll use the first 10 thresholds to match the 10 mAP values
sigmas_for_plot = sigmas[:10]

# Compute the scale factor so that the maximum (0.81) becomes 0.44
scale_factor = 0.44 / 0.81

# Scale all mAP values
scaled_mAP = [val * scale_factor for val in mAP_values]

plt.figure(figsize=(6, 4))
plt.plot(sigmas_for_plot, scaled_mAP, marker='o')
plt.title('mAP vs Sigma')
plt.xlabel('Sigma')
plt.ylabel('Scaled mAP')
plt.grid(True)
plt.tight_layout()
plt.show()

